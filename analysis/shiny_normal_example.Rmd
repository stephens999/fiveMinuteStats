---
title: 'Simple example: Bayesian inference for normal mean (known variance)'
author: "Nan Xiao"
date: 2016-04-13
---
**Last updated:** `r Sys.Date()`

**Code version:** `r system("git log -1 --format='%H'", intern = TRUE)`

```{r chunk-options, include=FALSE}
source("chunk-options.R")
```

This illustrates how the prior, likelihood, and posterior behave for
inference for a normal mean ($\mu$) from normal-distributed data, with
a conjugate prior on $\mu$.

Specifically the prior on $\mu$ is N($\mu_0$, $\tau_0^2$) [dotted line] and
the data is sampled from a normal distribution N($\mu$, $\sigma^2$),
which gives the likelihood [black line]. Note that the likelihood is scaled so
it fits nicely on the graph (remember, likelihoods only
matter up to a constant, so you can scale them however is convenient).

Because the normal distribution is the conjugate prior for normal sampling,
the posterior distribution is also a normal distribution, and is shown in red.

By Bayes theorem:

$$
\text{Pr}(\mu \, | \, \mathbf{y}, \sigma^2) \propto \text{Pr}(\mathbf{y} \, | \, \mu, \sigma^2) \text{Pr}(\mu)
$$

$$
\text{N}(\mu_1, \tau_1^2) = \text{N}(\mu, \sigma^2) \text{N}(\mu_0, \tau_0^2)
$$

where the posterior mean:

$$
\mu_1 = \frac{\frac{\mu_0}{\tau_0^2} + \frac{n \bar{y}}{\sigma^2}}{\frac{1}{\tau_0^2} + \frac{n}{\sigma^2}}
$$

and the posterior variance:

$$
\tau_1^2 = (\frac{1}{\tau_0^2} + \frac{n}{\sigma^2})^{-1}
$$

An interactive app that shows how the posterior distribution will change
when the prior and the (scaled) data likelihood changes:

<div style="text-align: center;"><iframe src="https://nanx.shinyapps.io/conjugate-normal-umkv/" frameborder="0" width="960" height="400"></iframe></div>

The source code of the app can be found [here](https://github.com/road2stat/conjugate-normal-umkv).

# Session information

```{r session-info}
```
