---
title: "Application on Dirichlet Process Model"
author: "Kaiqian Zhang"
date: "3/13/2018"
output: html_document
runtime: shiny
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Pre-requisites

You need to know how does Gibbs sampling work and what a Dirichlet process is.

## Application: Clustering Gaussian Mixture Data

Suppose we have data $X = \{x_1,\dots,x_n\}$ from a 10-dimensional Gaussian mixture model. The $k$th component has two parameters $\boldsymbol{\mu}_k$ and $\sum_k$ since each component is multi-normal distributed. We do not know the number of clusters. We will use Gibbs sampling (what we've learned before) to make inference on the number of clusters. 

Since we do not know the number of mixtures, we use Dirichlet process as our prior model. Here is our prior setting:

* $\beta_k \sim \text{Beta}(1,\alpha_0)$

* $\boldsymbol{\pi} = \text{Stick-Breaking-Process}(\boldsymbol{\beta})$

* $z_i | \boldsymbol{\pi} \sim \text{Multi}(\boldsymbol{\pi})$

* $\boldsymbol{\mu}_k \sim \text{Normal}(\boldsymbol{\mu}_0,\Sigma_0)$

* $\sum_k \sim \text{Inverse-Wishart}(\nu_0, \boldsymbol{\psi}_0)$

where $i=1\dots n$ and $k=1\dots$.

The conditional posteriors are: for the $s+1$th iteration,

* $\beta^{(s+1)}_k \sim \text{Beta}(1+n_k, \alpha_0+n_{k+})$ where $n_k$ denotes the number of data in the $k$th cluster and $n_{k+}$ is the sum of observations in the $k+1$, ..., up to $K$ cluster.

* $z_i^{(s+1)}\mid \boldsymbol{\pi}_{i}^{(s+1)},\boldsymbol{\mu}_k^{(s)}, \Sigma_k^{(s)},x_i  \sim \text{Multi}(\boldsymbol{\pi}_{i}^{(s+1)})$, where $\boldsymbol{\pi}_{i,k}^{(s+1)} = P(x_i |{\boldsymbol{\mu}_k}^{(s)}, \Sigma_k^{(s)})\cdot \boldsymbol{\pi}_{i,k}^{(s)}$. Here, $\boldsymbol{\pi}_{i,k}$ denotes the probability that $x_i$ is from the $k$th cluster.

* $\boldsymbol{\mu}_k^{(s+1)} \sim \text{Normal}((\Sigma_0^{-1}+n_k(\Sigma_k^{(s)})^{-1})^{-1} (\Sigma_0^{-1}\boldsymbol{\mu}_0 + n_k(\Sigma_k^{(s)})^{-1}\overline{x}),(\Sigma_0^{-1}+n_k(\Sigma_k^{(s)})^{-1})^{-1})$

* $\Sigma_k^{(s+1)} \sim \text{Inverse-Wishart}(n_k+\nu_0, \boldsymbol{\psi}_0+\sum_{i=1}^{n_k}(x_i-\boldsymbol{\mu}_k^{(s)})(x_i-\boldsymbol{\mu}_k^{(s)})^{T})$.

We write a Gibbs sampling algorithm based on the outline above, and we will simulate data and test our Gibbs sampling to see whether it can help find the number of mixtures.

We generate data from a 10-dimensional Gaussian mixture with six clusters. The proportion for this mixture is $50:25:100:50:50:25$. Here are our test data.

```{r echo=FALSE}
library(rmarkdown)
library(mvtnorm)
library(MASS)
library(MCMCpack)
```

```{r}
set.seed(12345)
mu1 = runif(10, -1, 0)
mu2 = runif(10, 5, 7)
mu3 = runif(10, 0, 5)
mu4 = runif(10, -5, 0)
mu5 = runif(10, 10, 15)
mu6 = runif(10, 15, 20)
Sigma1 = rWishart(1, 15, diag(1/15, 10))[,,1]
Sigma2 = rWishart(1, 15, diag(1/15, 10))[,,1]
Sigma3 = rWishart(1, 15, diag(1/15, 10))[,,1]
Sigma4 = rWishart(1, 15, diag(1/15, 10))[,,1]
Sigma5 = rWishart(1, 15, diag(1/15, 10))[,,1]
Sigma6 = rWishart(1, 15, diag(1/15, 10))[,,1]
Data = rbind(mvrnorm(50, mu1, Sigma1),
          mvrnorm(25, mu2, Sigma2),
          mvrnorm(100, mu3, Sigma3),
          mvrnorm(50, mu4, Sigma4),
          mvrnorm(50, mu5, Sigma5),
          mvrnorm(25, mu6, Sigma6))
```

Now suppose we do not know the number of clusters in our data. We propose that the number of mixtures might be 20 and our Gibbs sampling function will help us find the true number. 

```{r gibbs, echo=FALSE}
stick <- function(activeK, alpha, N){
  pi = rep(0, activeK)
  beta_vec = rep(0, activeK)
  for (k in 1:activeK){
    beta_vec[k] = rbeta(1,1+N[k], alpha+sum(N[k:activeK])-N[k])
  }
  pi[1] = beta_vec[1]
  for (i in 2:activeK){
    pi[i] = beta_vec[i]*prod(1-beta_vec[1:(i-1)])
  }
  pi
}

postProb <- function(my_data, pi, mu_list, sigma_list){
  activeK = length(mu_list)
  temp = pi
  post_pi = rep(0, length(pi))
  class_assignments = matrix(0, activeK, dim(my_data)[1])
  for (i in 1:dim(my_data)[1]){
    for (k in 1:activeK){
      mu_temp = mu_list[[k]]
      sigma_temp = sigma_list[[k]]
      temp[k] = pi[k]*dmvnorm(my_data[i,], mu_temp, sigma_temp)
    }
    post_pi = temp/sum(temp)
    Z = match(1, rmultinom(1, 1, post_pi))
    class_assignments[,i] = 0
    class_assignments[Z,i] = 1
  }
  class_assignments
}

updateMu <- function(p, subsetData, n_k, mu_0, lambda, sigma_k){
  if(length(subsetData)==p){
    xbar = subsetData
  }else{
    xbar = colMeans(subsetData)
  }
  mumean = (mu_0*lambda+n_k*xbar)/(lambda+n_k)
  muvar = 1/(lambda+n_k)*sigma_k
  res = mvrnorm(1, mumean, muvar)
  res
}

updateSigma <- function(p, subsetData, n_k, nu, psi, lambda, mu_0){
  first = nu + n_k
  if(length(subsetData)==p){
    xbar = subsetData
    second = psi + (lambda*n_k)/(lambda+n_k)*(xbar-mu_0)%*%t(xbar-mu_0)
    riwish(first, second)
  }else{
    xbar = colMeans(subsetData)
    ret = 0
    for (i in 1:n_k){
      temp = subsetData[i,]-xbar
      ret = ret + temp %*% t(temp)
    }
    second = psi + ret + (lambda*n_k)/(lambda+n_k)*(xbar-mu_0)%*%t(xbar-mu_0)
    riwish(first, second)
  }
}

Gibbs <- function(my_data, activeK, rep, nu, psi, mu_0, sigma_0, lambda){
  # Set-up
  p = ncol(my_data)
  pi = rep(1/activeK, activeK) # prior set-up
  mu_list = vector('list', activeK) # likelihood set-up
  sigma_list = vector('list', activeK)
  # Initialization
  kmeans_init = kmeans(my_data, activeK) # kmeans initialize mu and sigma
  mu_init_Mat = kmeans_init$centers
  for (i in 1:activeK){
    cluster_index = which(kmeans_init$cluster == i)
    cluster_index = as.vector(cluster_index)
    kth_cluster_data = my_data[cluster_index,]
    sigma_list[[i]] = cov(kth_cluster_data)
    mu_list[[i]] = mu_init_Mat[i,]
  }
  # Initial posterior Z-label matrix
  membership = postProb(my_data, pi, mu_list, sigma_list)
  N = rowSums(membership) # record n_k in each cluster
  print('Initial assignment:')
  print(N)
  
  # Identify active clusters so far (i.e: which clusters have more than one element, update only those)
  activeK_index = which(N>1)

  for (s in 1:rep){
    pi = stick(activeK,1,N) # prior
    membership = postProb(my_data, pi, mu_list, sigma_list)
    N = rowSums(membership)
    for (k in 1:activeK){
      if(N[k]>0){
        k_cluster_index = which(membership[k,] %in% 1)
        subsetData = my_data[k_cluster_index,]
        mu_k = updateMu(p, subsetData, N[k], mu_0, lambda, sigma_list[[k]])
        sigma_k = updateSigma(p, subsetData, N[k], nu, psi, lambda, mu_0)
        mu_list[[k]] = mu_k
        sigma_list[[k]] = sigma_k
      }else{
        mu_list[[k]] = mu_0
        sigma_list[[k]] = sigma_0
      }
    }
  }
  valid_group_index = which(N>0)
  final_mu_list = mu_list[valid_group_index]
  final_sigma_list = sigma_list[valid_group_index]
  groupings = c(1:activeK)%*%membership
  print('Final assignment: ')
  print(N)
  return(list('mu'=final_mu_list, 'sigma' = final_sigma_list, 'pi' = pi, 'membership' = groupings))
}
```


```{r}
result = Gibbs(Data, activeK=20, rep=300, nu=12, psi=diag(10,10), mu_0=rep(0,10), sigma_0=diag(10,10), lambda=1)
```

Our Gibbs sampling algorithm also makes inference on the $\boldsymbol{\mu}$ and $\Sigma$ for each cluster and they are amazingly close to the actual $\boldsymbol{\mu}$ and $\Sigma$. We show our estimates on $\boldsymbol{\mu}$ here. 

```{r}
result$mu
```
