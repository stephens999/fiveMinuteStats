<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">

<head>

<meta charset="utf-8">
<meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
<meta name="generator" content="pandoc" />


<meta name="author" content="Matt Bonakdarpour" />

<meta name="date" content="2016-01-21" />

<title>Simulating Discrete Markov Chains: Limiting Disributions</title>

<script src="site_libs/jquery-1.11.3/jquery.min.js"></script>
<meta name="viewport" content="width=device-width, initial-scale=1" />
<link href="site_libs/bootstrap-3.3.5/css/readable.min.css" rel="stylesheet" />
<script src="site_libs/bootstrap-3.3.5/js/bootstrap.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/html5shiv.min.js"></script>
<script src="site_libs/bootstrap-3.3.5/shim/respond.min.js"></script>
<script src="site_libs/jqueryui-1.11.4/jquery-ui.min.js"></script>
<link href="site_libs/tocify-1.9.1/jquery.tocify.css" rel="stylesheet" />
<script src="site_libs/tocify-1.9.1/jquery.tocify.js"></script>
<script src="site_libs/navigation-1.1/tabsets.js"></script>
<link href="site_libs/highlightjs-1.1/textmate.css" rel="stylesheet" />
<script src="site_libs/highlightjs-1.1/highlight.js"></script>
<link href="site_libs/font-awesome-4.5.0/css/font-awesome.min.css" rel="stylesheet" />

<style type="text/css">code{white-space: pre;}</style>
<style type="text/css">
  pre:not([class]) {
    background-color: white;
  }
</style>
<script type="text/javascript">
if (window.hljs && document.readyState && document.readyState === "complete") {
   window.setTimeout(function() {
      hljs.initHighlighting();
   }, 0);
}
</script>



<style type="text/css">
h1 {
  font-size: 34px;
}
h1.title {
  font-size: 38px;
}
h2 {
  font-size: 30px;
}
h3 {
  font-size: 24px;
}
h4 {
  font-size: 18px;
}
h5 {
  font-size: 16px;
}
h6 {
  font-size: 12px;
}
.table th:not([align]) {
  text-align: left;
}
</style>


</head>

<body>

<style type = "text/css">
.main-container {
  max-width: 940px;
  margin-left: auto;
  margin-right: auto;
}
code {
  color: inherit;
  background-color: rgba(0, 0, 0, 0.04);
}
img {
  max-width:100%;
  height: auto;
}
.tabbed-pane {
  padding-top: 12px;
}
button.code-folding-btn:focus {
  outline: none;
}
</style>


<style type="text/css">
/* padding for bootstrap navbar */
body {
  padding-top: 66px;
  padding-bottom: 40px;
}
/* offset scroll position for anchor links (for fixed navbar)  */
.section h1 {
  padding-top: 71px;
  margin-top: -71px;
}

.section h2 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h3 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h4 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h5 {
  padding-top: 71px;
  margin-top: -71px;
}
.section h6 {
  padding-top: 71px;
  margin-top: -71px;
}
</style>

<script>
// manage active state of menu based on current page
$(document).ready(function () {
  // active menu anchor
  href = window.location.pathname
  href = href.substr(href.lastIndexOf('/') + 1)
  if (href === "")
    href = "index.html";
  var menuAnchor = $('a[href="' + href + '"]');

  // mark it active
  menuAnchor.parent().addClass('active');

  // if it's got a parent navbar menu mark it active as well
  menuAnchor.closest('li.dropdown').addClass('active');
});
</script>


<div class="container-fluid main-container">

<!-- tabsets -->
<script>
$(document).ready(function () {
  window.buildTabsets("TOC");
});
</script>

<!-- code folding -->




<script>
$(document).ready(function ()  {

    // move toc-ignore selectors from section div to header
    $('div.section.toc-ignore')
        .removeClass('toc-ignore')
        .children('h1,h2,h3,h4,h5').addClass('toc-ignore');

    // establish options
    var options = {
      selectors: "h1,h2,h3",
      theme: "bootstrap3",
      context: '.toc-content',
      hashGenerator: function (text) {
        return text.replace(/[.\\/?&!#<>]/g, '').replace(/\s/g, '_').toLowerCase();
      },
      ignoreSelector: ".toc-ignore",
      scrollTo: 0
    };
    options.showAndHide = true;
    options.smoothScroll = true;

    // tocify
    var toc = $("#TOC").tocify(options).data("toc-tocify");
});
</script>

<style type="text/css">

#TOC {
  margin: 25px 0px 20px 0px;
}
@media (max-width: 768px) {
#TOC {
  position: relative;
  width: 100%;
}
}


.toc-content {
  padding-left: 30px;
  padding-right: 40px;
}

div.main-container {
  max-width: 1200px;
}

div.tocify {
  width: 20%;
  max-width: 260px;
  max-height: 85%;
}

@media (min-width: 768px) and (max-width: 991px) {
  div.tocify {
    width: 25%;
  }
}

@media (max-width: 767px) {
  div.tocify {
    width: 100%;
    max-width: none;
  }
}

.tocify ul, .tocify li {
  line-height: 20px;
}

.tocify-subheader .tocify-item {
  font-size: 0.90em;
  padding-left: 25px;
  text-indent: 0;
}

.tocify .list-group-item {
  border-radius: 0px;
}


</style>

<!-- setup 3col/9col grid for toc_float and main content  -->
<div class="row-fluid">
<div class="col-xs-12 col-sm-4 col-md-3">
<div id="TOC" class="tocify">
</div>
</div>

<div class="toc-content col-xs-12 col-sm-8 col-md-9">




<div class="navbar navbar-default  navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar">
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <a class="navbar-brand" href="index.html">fiveMinuteStats</a>
    </div>
    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
        <li>
  <a href="index.html">Home</a>
</li>
<li>
  <a href="about.html">About</a>
</li>
<li>
  <a href="license.html">License</a>
</li>
      </ul>
      <ul class="nav navbar-nav navbar-right">
        <li>
  <a href="https://github.com/stephens999/fiveMinuteStats">
    <span class="fa fa-github"></span>
     
  </a>
</li>
      </ul>
    </div><!--/.nav-collapse -->
  </div><!--/.container -->
</div><!--/.navbar -->

<div class="fluid-row" id="header">



<h1 class="title toc-ignore">Simulating Discrete Markov Chains: Limiting Disributions</h1>
<h4 class="author"><em>Matt Bonakdarpour</em></h4>
<h4 class="date"><em>2016-01-21</em></h4>

</div>


<!-- The file analysis/chunks.R contains chunks that define default settings
shared across the workflowr files. -->
<!-- Update knitr chunk options -->
<!-- Insert the date the file was last updated -->
<p><strong>Last updated:</strong> 2017-03-06</p>
<!-- Insert the code version (Git commit SHA1) if Git repository exists and R
 package git2r is installed -->
<p><strong>Code version:</strong> c7339fc</p>
<div id="pre-requisites" class="section level1">
<h1>Pre-requisites</h1>
<p>This document assumes basic familiarity with simulating Markov chains, as seen here: <a href="simulating_discrete_chains_1.html">Simulating Discrete Markov Chains: An Introduction</a>.</p>
</div>
<div id="overview" class="section level1">
<h1>Overview</h1>
<p>In this note, we show the empirical relationship between the stationary distribution, limiting probabilities, and empirical probabilities for discrete Markov chains.</p>
</div>
<div id="limiting-probabilities" class="section level1">
<h1>Limiting Probabilities</h1>
<p>Let <span class="math inline">\(\pi^{(0)}\)</span> be our initial probability vector. For example, if we had a 3 state Markov chain with <span class="math inline">\(\pi^{(0)} = [0.5, 0.1, 0.4]\)</span>, this would tell us that our chain has a 50% probability of starting in state 1, a 10% probability of starting in state 2, and a 40% probability of starting in state 3. If we wanted to have our initial state equal to 1, we would set <span class="math inline">\(\pi^{(0)} = [1, 0, 0]\)</span>.</p>
<p>Let <span class="math inline">\(P\)</span> be our probability transition matrix. Recall that the probability vector after <span class="math inline">\(n\)</span> steps is equal to: <span class="math display">\[\pi^{(n)} = \pi^{(0)}P^n\]</span> where <span class="math inline">\(P^n\)</span> is the matrix <span class="math inline">\(P\)</span> raised to the <span class="math inline">\(n\)</span>-th power.</p>
<p>With the facts above, we could keep track of our probability vector <span class="math inline">\(\pi^{(n)}\)</span> as we simulate the Markov chain as follows:</p>
<ol style="list-style-type: decimal">
<li>Obtain probability transition matrix <span class="math inline">\(P\)</span><br />
</li>
<li>Set <span class="math inline">\(P_n = P\)</span><br />
</li>
<li>For t = 1…T:
<ol style="list-style-type: decimal">
<li>Simulate next step of Markov chain.</li>
<li>Set <span class="math inline">\(\pi^{(n)} = \pi^{(0)}P_n\)</span><br />
</li>
<li>Set <span class="math inline">\(P_n = P_nP\)</span></li>
</ol></li>
</ol>
<p>We augment our function for simulating Markov chains from <a href="simulating_discrete_chains_1.html">this note</a> with the following changes:<br />
1. We keep track of <span class="math inline">\(\pi^{(n)}\)</span>.<br />
2. We add the functionality of simulating multiple chains at once – as a result, the vectors we dealt with in the previous note have turned into matrices.<br />
3. We use the <code>rmultinom()</code> function instead of our <code>inv.transform.sample()</code> method.</p>
<pre class="r"><code># simulate discrete Markov chains
run.mc.sim &lt;- function( P,   # probability transition matrix
                        num.iters=50, 
                        num.chains=150 )
  {
  
  # number of possible states
  num.states &lt;- nrow(P)
  
  # states X_t for all chains
  states     &lt;- matrix(NA, ncol=num.chains, nrow=num.iters)
  
  # probability vectors pi^n through time
  all_probs  &lt;- matrix(NA, nrow=num.iters, ncol=num.states)
  
  # forces chains to start in state 1
  pi_0      &lt;- c(1, rep(0, num.states-1))

  # initialize variables for first state 
  P_n           &lt;- P
  all_probs[1,] &lt;- pi_0
  states[1,]    &lt;- 1

  for(t in 2:num.iters) {
    
    # pi^n for this iteration
    pi_n           &lt;- pi_0 %*% P_n
    all_probs[t,]  &lt;- pi_n
    
    for(chain_num in seq_len(num.chains)) {
      # probability vector to simulating next state 
      p                     &lt;- P[ states[t-1,chain_num], ]
      states[t,chain_num]   &lt;- which(rmultinom(1, 1, p) == 1)
    }
    
    # update probability transition matrix
    P_n           &lt;- P_n %*% P
  }
  return(list(all.probs=all_probs, states=states))
}</code></pre>
</div>
<div id="simulation-1-3x3-example" class="section level1">
<h1>Simulation 1: 3x3 example</h1>
<p>Assume our probability transition matrix is: <span class="math display">\[P = \begin{bmatrix}
    0.7 &amp; 0.2 &amp; 0.1 \\
    0.4 &amp; 0.6 &amp; 0 \\
    0   &amp; 1   &amp; 0 
\end{bmatrix}\]</span></p>
<p>We initialize this matrix in R below:</p>
<pre class="r"><code># setup transition matrix 
P &lt;- t(matrix(c( 0.7, 0.2, 0.1, 
                 0.4, 0.6,   0, 
                   0,   1,   0  ), nrow=3, ncol=3))</code></pre>
<p>We will inspect the limiting probabilities and compare them to the stationary distribution. In <a href="stationary_distribution.html">this note</a>, we derived the stationary distribution for this transition matrix. Recall that the stationary distribution <span class="math inline">\(\pi\)</span> is the vector such that <span class="math display">\[\pi = \pi P\]</span></p>
<p>We found that: <span class="math display">\[\begin{align*}
\pi_1 \approx 0.54, \pi_2 \approx 0.41, \pi_3 \approx 0.05
\end{align*}\]</span></p>
<p>Therefore, with proper conditions (see below), we expect the Markov chain to spend more time in states 1 and 2 as the chain evolves.</p>
<p>Now we will use the function we wrote in the previous section to check this result numerically.</p>
<pre class="r"><code>sim1 &lt;- run.mc.sim(P)</code></pre>
<p>Our function returns a list containing two matrices. The second matrix called “states” contains the states of each of our simulated chains through time. Recall that our state space is <span class="math inline">\(\{1,2,3\}\)</span>. Below, we first visualize how 5 of these chains evolve:</p>
<pre class="r"><code>states &lt;- sim1[[2]]
matplot(states[,1:5], type=&#39;l&#39;, lty=1, col=1:5, ylim=c(0,4), ylab=&#39;state&#39;, xlab=&#39;time&#39;)
abline(h=1, lty=3)
abline(h=3, lty=3)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-4-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>The first matrix we get from our function contains <span class="math inline">\(\pi^{(n)}\)</span> through time. We can see how <span class="math inline">\(\pi^{(n)}\)</span> evolves as <span class="math inline">\(n\)</span> grows, and we can check if it converges to the stationary distribution we found above. For irreducible finite state Markov chains, note that <span class="math inline">\(\pi^{(n)}\)</span> converges if and only if the Markov chain is aperiodic. In this note, we only consider finite, irreducible, and aperiodic Markov chains.</p>
<p>Using <span class="math inline">\(\pi^{(n)}\)</span>, we plot the time evolution of the probability of being in each state:</p>
<pre class="r"><code>all.probs &lt;- sim1[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:3, lty=1, ylab=&#39;probability&#39;, xlab=&#39;time&#39;)
legend(&#39;topright&#39;, c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;), lty=1, col=1:3)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-5-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Indeed, we see that these probabilities quickly converge. Just by looking at the plot, we can see that the final probabilities are about equal to the stationary distribution <span class="math inline">\(\pi\)</span> we found above.</p>
<p>By inspecting the actual values, we confirm that the values of <span class="math inline">\(\pi^{(n)}\)</span> converge to the vector <span class="math inline">\(\pi\)</span> exactly. The first row in the matrix below is from the simulation, and the second row is the quantity we obtained by solving for the stationary distribution:</p>
<pre class="r"><code># solve for stationary distribution
A         &lt;- matrix(c(-0.3, 0.2, 0.1, 1, 0.4, -0.4, 0, 1, 0, 1, -1, 1 ), ncol=3,nrow=4)
b         &lt;- c(0,0,0, 1)
pi        &lt;- drop(solve(t(A) %*% A, t(A) %*% b))

# show comparison
results1           &lt;- t(data.frame(pi_n = all.probs[50,], pi = pi))
colnames(results1) &lt;- c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;)
results1</code></pre>
<pre><code>       state.1   state.2    state.3
pi_n 0.5405405 0.4054054 0.05405405
pi   0.5405405 0.4054054 0.05405405</code></pre>
<p>Finally, we can also plot the proportion of chains that are in each state through time. These should roughly equal the probability vectors above, with some noise due to random chance:</p>
<pre class="r"><code>state.probs &lt;- t(apply(apply(sim1[[2]], 1, function(x) table(factor(x, levels=1:3))), 2, function(x) x/sum(x)))
matplot(state.probs[1:50,], col=1:3, lty=1, type=&#39;l&#39;, ylab=&#39;empirical probability&#39;, xlab=&#39;time&#39;)
legend(&#39;topright&#39;, c(&#39;state.1&#39;, &#39;state.2&#39;, &#39;state.3&#39;), lty=1, col=1:3)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-7-1.png" width="672" style="display: block; margin: auto;" /></p>
</div>
<div id="simulation-2-8x8-example" class="section level1">
<h1>Simulation 2: 8x8 example</h1>
<p>Next we will quickly do two larger experiments with the size of our state space equal to 8. Assume our probability transition matrix is: <span class="math display">\[P = \begin{bmatrix}
    0.33 &amp; 0.66 &amp; 0     &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0.33 &amp; 0.33 &amp; 0.33  &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0.33 &amp; 0.33 &amp; 0.33 &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0.33 &amp; 0.33 &amp; 0.33 &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0    &amp; 0.33 &amp; 0.33 &amp; 0.33  &amp; 0    &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33 &amp; 0.33  &amp; 0.33 &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33  &amp; 0.33 &amp; 0.33 \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0     &amp; 0.66 &amp; 0.33 \\
\end{bmatrix}\]</span></p>
<p>We first initialize our transition matrix in R:</p>
<pre class="r"><code>P &lt;- t(matrix(c( 1/3, 2/3,   0,   0,  0,   0,   0,   0,
                 1/3, 1/3, 1/3,   0,  0,   0,   0,   0,
                   0, 1/3, 1/3, 1/3,  0,   0,   0,   0,
                   0,   0, 1/3, 1/3, 1/3,  0,   0,   0,
                   0,   0,   0, 1/3, 1/3, 1/3,  0,   0,
                   0,   0,   0,   0, 1/3, 1/3, 1/3,  0,
                   0,   0,   0,   0,   0, 1/3, 1/3, 1/3,
                   0,   0,   0,   0,   0,   0, 2/3, 1/3), nrow=8, ncol=8))</code></pre>
<p>After briefly studying this matrix, we can see that for states 2 through 7, this transition matrix forces the chain to either stay in the current state or move one state up or down, all with equal probability. For the edge cases, states 1 and 8, the chain can either stay or reflect towards the middle states. Since it’s “easier” to get to one of the middle states (either from above or below), we should see that the probabilities for these states converge to a higher number than the states on the boundaries.</p>
<p>Now we run our simulation with the transition matrix above:</p>
<pre class="r"><code>sim2a &lt;- run.mc.sim(P)</code></pre>
<p>and now plot 5 of the chains through time below:</p>
<pre class="r"><code>states &lt;- sim2a[[2]]
matplot(states[,1:5], type=&#39;l&#39;, lty=1, col=1:5, ylim=c(0,9), ylab=&#39;state&#39;, xlab=&#39;time&#39;)
abline(h=1, lty=3)
abline(h=8, lty=3)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-10-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Next we inspect <span class="math inline">\(\pi^{(n)}\)</span> through time:</p>
<pre class="r"><code>all.probs &lt;- sim2a[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:8, lty=1, ylab=&#39;probability&#39;,
        xlab=&#39;time&#39;, ylim=c(0, 0.5))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-11-1.png" width="672" style="display: block; margin: auto;" /> These results match our intuition above. The probability of being in states 1 and 8 converge to smaller values than the others.</p>
<p>Now we alter the transition matrix above to encourage the chain to stay in states 4 and 5: <span class="math display">\[P = \begin{bmatrix}
    0.33 &amp; 0.66 &amp; 0     &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0.33 &amp; 0.33 &amp; 0.33  &amp; 0   &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0.08 &amp; 0.08 &amp; 0.84 &amp; 0    &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0.08 &amp; 0.84 &amp; 0.08 &amp; 0     &amp; 0    &amp; 0 \\
    0    &amp; 0    &amp; 0    &amp; 0.08 &amp; 0.84 &amp; 0.08  &amp; 0    &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.84 &amp; 0.08  &amp; 0.08 &amp; 0   \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0.33  &amp; 0.33 &amp; 0.33 \\
    0    &amp; 0    &amp; 0    &amp; 0    &amp; 0    &amp; 0     &amp; 0.66 &amp; 0.33 \\
\end{bmatrix}\]</span></p>
<p>and initialize the transition matrix in R:</p>
<pre class="r"><code>P &lt;- t(matrix(c( 1/3,   2/3,    0,    0,    0,    0,    0,   0,
                 1/3,   1/3,  1/3,    0,    0,    0,    0,   0,
                   0,  .5/6, .5/6,  5/6,    0,    0,    0,   0,
                   0,     0, .5/6,  5/6, .5/6,    0,    0,   0,
                   0,     0,    0, .5/6,  5/6, .5/6,    0,   0,
                   0,     0,    0,    0,  5/6, .5/6, .5/6,   0,
                   0,     0,    0,    0,    0,  1/3,  1/3, 1/3,
                   0,     0,    0,    0,    0,    0,  2/3, 1/3 ), nrow=8, ncol=8))</code></pre>
<pre class="r"><code>sim2b &lt;- run.mc.sim(P)</code></pre>
<p>Below we inspect <span class="math inline">\(\pi^{(n)}\)</span> through time and see that the probability vector converges to a vector placing most of the probability mass on states 4 and 5.</p>
<pre class="r"><code>all.probs &lt;- sim2b[[1]]
matplot(all.probs, type=&#39;l&#39;, col=1:8, lty=1, ylab=&#39;probability&#39;,
        xlab=&#39;time&#39;, ylim=c(0,1.2))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>Finally we confirm that our empirical probabilities also exhibit similar behavior:</p>
<pre class="r"><code>state.probs &lt;- t(apply(apply(sim2b[[2]], 1, function(x) table(factor(x, levels=1:8))), 2, function(x) x/sum(x)))
matplot(state.probs[1:50,], col=1:8, lty=1, type=&#39;l&#39;, ylab=&#39;empirical probability&#39;, xlab=&#39;time&#39;, ylim=c(0,1.2))
legend(&#39;topright&#39;, paste(&#39;state.&#39;, 1:8, sep=&#39;&#39;), lty=1, col=1:8)</code></pre>
<p><img src="figure/simulating_discrete_chains_2.Rmd/unnamed-chunk-15-1.png" width="672" style="display: block; margin: auto;" /></p>
<div id="session-information" class="section level2">
<h2>Session information</h2>
<!-- Insert the session information into the document -->
<pre class="r"><code>sessionInfo()</code></pre>
<pre><code>R version 3.3.2 (2016-10-31)
Platform: x86_64-pc-linux-gnu (64-bit)
Running under: Ubuntu 14.04.5 LTS

locale:
 [1] LC_CTYPE=en_US.UTF-8       LC_NUMERIC=C              
 [3] LC_TIME=en_US.UTF-8        LC_COLLATE=en_US.UTF-8    
 [5] LC_MONETARY=en_US.UTF-8    LC_MESSAGES=en_US.UTF-8   
 [7] LC_PAPER=en_US.UTF-8       LC_NAME=C                 
 [9] LC_ADDRESS=C               LC_TELEPHONE=C            
[11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C       

attached base packages:
[1] stats     graphics  grDevices utils     datasets  methods   base     

other attached packages:
[1] knitr_1.15.1       MASS_7.3-45        expm_0.999-0      
[4] Matrix_1.2-8       workflowr_0.4.0    rmarkdown_1.3.9004

loaded via a namespace (and not attached):
 [1] Rcpp_0.12.9     lattice_0.20-34 gtools_3.5.0    digest_0.6.12  
 [5] rprojroot_1.2   mime_0.5        R6_2.2.0        grid_3.3.2     
 [9] xtable_1.8-2    backports_1.0.5 git2r_0.18.0    magrittr_1.5   
[13] evaluate_0.10   stringi_1.1.2   tools_3.3.2     stringr_1.2.0  
[17] shiny_1.0.0     httpuv_1.3.3    yaml_2.1.14     htmltools_0.3.5</code></pre>
</div>
</div>

<hr>
<p>
    This site was created with <a href="http://rmarkdown.rstudio.com">R Markdown</a>
</p>
<hr>

<!-- To enable disqus, uncomment the section below and provide your disqus_shortname -->

<!-- disqus
  <div id="disqus_thread"></div>
    <script type="text/javascript">
        /* * * CONFIGURATION VARIABLES: EDIT BEFORE PASTING INTO YOUR WEBPAGE * * */
        var disqus_shortname = 'rmarkdown'; // required: replace example with your forum shortname

        /* * * DON'T EDIT BELOW THIS LINE * * */
        (function() {
            var dsq = document.createElement('script'); dsq.type = 'text/javascript'; dsq.async = true;
            dsq.src = '//' + disqus_shortname + '.disqus.com/embed.js';
            (document.getElementsByTagName('head')[0] || document.getElementsByTagName('body')[0]).appendChild(dsq);
        })();
    </script>
    <noscript>Please enable JavaScript to view the <a href="http://disqus.com/?ref_noscript">comments powered by Disqus.</a></noscript>
    <a href="http://disqus.com" class="dsq-brlink">comments powered by <span class="logo-disqus">Disqus</span></a>
-->


</div>
</div>

</div>

<script>

// add bootstrap table styles to pandoc tables
function bootstrapStylePandocTables() {
  $('tr.header').parent('thead').parent('table').addClass('table table-condensed');
}
$(document).ready(function () {
  bootstrapStylePandocTables();
});


</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    script.src  = "https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML";
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

</body>
</html>
